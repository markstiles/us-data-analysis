{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs-Z_h-5kGi0",
        "outputId": "45d6cfe3-b94c-4cf7-95c0-600a83ad0d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install torchviz -q\n",
        "! pip install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aUldvkRv9R-c"
      },
      "outputs": [],
      "source": [
        "# namespaces\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'notebook'\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import joblib\n",
        "import math\n",
        "\n",
        "# functions\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "from google.colab import files\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ISwmp2Cx8ch2"
      },
      "outputs": [],
      "source": [
        "# get data, drop index column and zero the blanks\n",
        "places_df = pd.read_csv('https://raw.githubusercontent.com/markstiles/us-data-analysis/main/places_data.csv')\n",
        "places_df.drop(places_df.columns[[0]], axis=1, inplace=True)\n",
        "places_df = places_df.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fkwKah1P8ZKf"
      },
      "outputs": [],
      "source": [
        "# build a one-hot by state and attach it\n",
        "state_dummies = pd.get_dummies(places_df.State_Name)\n",
        "places_df = pd.concat([places_df, state_dummies.set_axis(places_df.index)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "eBf7fKnrCpdo"
      },
      "outputs": [],
      "source": [
        "#@title Drop Column List\n",
        "# get rid of the locations and sparse data\n",
        "drop_cols = [\n",
        " 'Place_Name', 'State_Abbr', 'State_Name', 'Type', 'All_Employers', 'All_Employees', 'All_Payroll',\n",
        " 'Agriculture_Employers','Agriculture_Employees','Agriculture_Payroll','Agriculture_Revenue',\n",
        " 'Construction_Employers','Construction_Employees','Construction_Payroll','Construction_Revenue',\n",
        " 'Finance_Revenue',\n",
        " 'Information_Revenue',\n",
        " 'Management_Employers','Management_Employees','Management_Payroll','Management_Revenue',\n",
        " 'Manufacturing_Employers','Manufacturing_Employees','Manufacturing_Payroll','Manufacturing_Revenue',\n",
        " 'Mining_Employers','Mining_Employees','Mining_Payroll','Mining_Revenue',\n",
        " 'Utilities_Revenue',\n",
        " 'Wholesale_Employers','Wholesale_Employees','Wholesale_Payroll','Wholesale_Revenue',\n",
        " 'Consumer_Total_Expense','Consumer_Expense_Alcohol','Consumer_Expense_Alcohol_Home','Consumer_Expense_Beer_Bar','Consumer_Expense_Wine_Bar','Consumer_Expense_Clothes','Consumer_Expense_Mens_Clothes',\n",
        " 'Consumer_Expense_Womens_Clothes','Consumer_Expense_Childrens_Clothes','Consumer_Expense_Boys_Clothes','Consumer_Expense_Girls_Clothes','Consumer_Expense_Footwear','Consumer_Expense_Dining',\n",
        " 'Consumer_Expense_Dining_Breakfast','Consumer_Expense_Dining_Lunch','Consumer_Expense_Dining_Dinner','Consumer_Expense_Education','Consumer_Expense_Entertainment','Consumer_Expense_Clubs',\n",
        " 'Consumer_Expense_Dating','Consumer_Expense_Pet_Food','Consumer_Expense_Pet_Services','Consumer_Expense_Food_Home','Consumer_Expense_Bakery_Home','Consumer_Expense_Dairy_Home','Consumer_Expense_Fruits_Home',\n",
        " 'Consumer_Expense_Meat_Home','Consumer_Expense_Nonalcohol_Home','Consumer_Expense_Snacks_Home','Consumer_Expense_Healthcare','Consumer_Expense_Mentalcare','Consumer_Expense_Drugcar','Consumer_Expense_House_Services',\n",
        " 'Consumer_Expense_Eldercare','Consumer_Expense_Landscape','Consumer_Expense_Housekeeping','Consumer_Expense_PC','Consumer_Expense_Housing','Consumer_Expense_Home_Improvements','Consumer_Expense_Energy',\n",
        " 'Consumer_Expense_Phone','Consumer_Expense_Water','Consumer_Expense_Insurance','Consumer_Expense_Pensions','Consumer_Expense_Personalcare','Consumer_Expense_Haircare','Consumer_Expense_Personalcare_Products',\n",
        " 'Consumer_Expense_Transport','Consumer_Expense_Gas','Consumer_Expense_Vehicle_Repair','Consumer_Expense_Travel','Consumer_Expense_Airfare','Consumer_Expense_Auto_Rentals','Consumer_Expense_Travel_Lodging',\n",
        " 'Consumer_Expense_Travel_Meals','Consumer_Expense_Travel_Entertainment',\n",
        " 'All_Employee_Per_Employer','All_Revenue_Per_Employer','All_Avg_Payroll_Per_Employee','All_Population_Per_Employer',\n",
        " 'Food_Services_Employee_Per_Employer','Food_Services_Revenue_Per_Employer','Food_Services_Avg_Payroll_Per_Employee','Food_Services_Population_Per_Employer','Waste_Management_Employee_Per_Employer',\n",
        " 'Waste_Management_Revenue_Per_Employer','Waste_Management_Avg_Payroll_Per_Employee','Waste_Management_Population_Per_Employer','Agriculture_Employee_Per_Employer','Agriculture_Revenue_Per_Employer',\n",
        " 'Agriculture_Avg_Payroll_Per_Employee','Agriculture_Population_Per_Employer','Arts_Employee_Per_Employer','Arts_Revenue_Per_Employer','Arts_Avg_Payroll_Per_Employee','Arts_Population_Per_Employer',\n",
        " 'Construction_Employee_Per_Employer','Construction_Revenue_Per_Employer','Construction_Avg_Payroll_Per_Employee','Construction_Population_Per_Employer','Education_Employee_Per_Employer',\n",
        " 'Education_Revenue_Per_Employer','Education_Avg_Payroll_Per_Employee','Education_Population_Per_Employer','Finance_Employee_Per_Employer','Finance_Revenue_Per_Employer','Finance_Avg_Payroll_Per_Employee',\n",
        " 'Finance_Population_Per_Employer','Healthcare_Employee_Per_Employer','Healthcare_Revenue_Per_Employer','Healthcare_Avg_Payroll_Per_Employee','Healthcare_Population_Per_Employer','Information_Employee_Per_Employer',\n",
        " 'Information_Revenue_Per_Employer','Information_Avg_Payroll_Per_Employee','Information_Population_Per_Employer','Management_Employee_Per_Employer','Management_Revenue_Per_Employer','Management_Avg_Payroll_Per_Employee',\n",
        " 'Management_Population_Per_Employer','Manufacturing_Employee_Per_Employer','Manufacturing_Revenue_Per_Employer','Manufacturing_Avg_Payroll_Per_Employee','Manufacturing_Population_Per_Employer','Mining_Employee_Per_Employer',\n",
        " 'Mining_Revenue_Per_Employer','Mining_Avg_Payroll_Per_Employee','Mining_Population_Per_Employer','Other_Employee_Per_Employer','Other_Revenue_Per_Employer','Other_Avg_Payroll_Per_Employee','Other_Population_Per_Employer',\n",
        " 'Technical_Employee_Per_Employer','Technical_Revenue_Per_Employer','Technical_Avg_Payroll_Per_Employee','Technical_Population_Per_Employer','Real_Estate_Employee_Per_Employer','Real_Estate_Revenue_Per_Employer',\n",
        " 'Real_Estate_Avg_Payroll_Per_Employee','Real_Estate_Population_Per_Employer','Retail_Employee_Per_Employer','Retail_Revenue_Per_Employer','Retail_Avg_Payroll_Per_Employee','Retail_Population_Per_Employer',\n",
        " 'Transportation_Employee_Per_Employer','Transportation_Revenue_Per_Employer','Transportation_Avg_Payroll_Per_Employee','Transportation_Population_Per_Employer','Utilities_Employee_Per_Employer','Utilities_Revenue_Per_Employer',\n",
        " 'Utilities_Avg_Payroll_Per_Employee','Utilities_Population_Per_Employer','Wholesale_Employee_Per_Employer','Wholesale_Revenue_Per_Employer','Wholesale_Avg_Payroll_Per_Employee','Wholesale_Population_Per_Employer',\n",
        " 'Income_Per_Revenue',\n",
        " 'Revenue_Per_Person','Profit_Per_Person',\n",
        " 'Population_Range', 'Performance'\n",
        "]\n",
        "places_df.drop(columns=drop_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kJi7oGtH9QYd"
      },
      "outputs": [],
      "source": [
        "# drop the top skewing outliers\n",
        "places_df = places_df[places_df.All_Revenue < 20000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_tAU7mwGT0K",
        "outputId": "4634ca6a-3171-4eec-9c0b-4195a9f5148e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9775, 174)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# set the y to revenue\n",
        "y = places_df['All_Revenue'].to_numpy()\n",
        "# remove the identifier and target columns\n",
        "x = places_df.drop(columns=['GeoId', 'All_Revenue'])\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mr9DFVB-Wp08"
      },
      "outputs": [],
      "source": [
        "# train-test split for model evaluation\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, shuffle=True)\n",
        "\n",
        "# normalize\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "\n",
        "# scale the data\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# convert to 2d pytorch tensors\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yie30pB_HIED"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x.shape[1], 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 1)\n",
        ")\n",
        "\n",
        "# loss function and optimizer\n",
        "loss_fn = nn.MSELoss()  # mean square error\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wUhcWE73S4aZ"
      },
      "outputs": [],
      "source": [
        "# hold the best model\n",
        "best_mse = np.inf   # init to infinity\n",
        "best_weights = None\n",
        "train_history = []\n",
        "test_history = []\n",
        "\n",
        "n_epochs = 10000   # number of epochs to run\n",
        "batch_size = 500  # size of each batch\n",
        "batch_start = torch.arange(0, len(x_train), batch_size)\n",
        "step_count = 0\n",
        "step_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_DLk-V_S5SZ"
      },
      "outputs": [],
      "source": [
        "start_time = timer()\n",
        "\n",
        "# train and store progress\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for start in bar:\n",
        "            # take a batch\n",
        "            x_batch = x_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass\n",
        "            y_pred = model(x_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            mse = float(loss)\n",
        "            if mse < best_mse:\n",
        "              best_mse = mse\n",
        "              best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if step_count % step_size == 0:\n",
        "              train_history.append(loss.item())\n",
        "              with torch.no_grad():\n",
        "                  y_test_pred = model(x_test)\n",
        "                  test_loss = loss_fn(y_test_pred.float(), y_test)\n",
        "                  test_history.append(test_loss.item())\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # print progress\n",
        "            bar.set_postfix(mse=float(loss))\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "end_time = timer()\n",
        "print()\n",
        "print(timedelta(seconds=end_time-start_time))\n",
        "print(best_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGnRdRAFTRWa"
      },
      "outputs": [],
      "source": [
        "# restore model and return best accuracy\n",
        "model.load_state_dict(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn-QQiwNAoh1"
      },
      "outputs": [],
      "source": [
        "model_filename = 'places_model.pt'\n",
        "torch.save(model.state_dict(), model_filename)\n",
        "files.download(model_filename)\n",
        "\n",
        "scaler_filename = \"scaler.save\"\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "files.download(scaler_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8pAEhohTPtk"
      },
      "outputs": [],
      "source": [
        "blue_patch = mpatches.Patch(color = 'blue', label = 'Train MSE')\n",
        "orange_patch = mpatches.Patch(color = 'orange', label = 'Validation MSE')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "sns.lineplot(x=range(1,len(train_history)+1),y = train_history)\n",
        "sns.lineplot(x=range(1,len(test_history)+1),y = test_history)\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend(handles = [blue_patch,orange_patch])\n",
        "plt.title('Training and Validation loss');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIQKqlZVCm80"
      },
      "outputs": [],
      "source": [
        "# give it a test on my hometown\n",
        "waltham = places_df[places_df.GeoId==2572600]\n",
        "actual = waltham.iloc[0].All_Revenue\n",
        "waltham_scaled = scaler.transform(waltham.drop(columns=['GeoId', 'All_Revenue']))\n",
        "prediction = model(torch.tensor(waltham_scaled, dtype=torch.float32)).item()\n",
        "print(f'actual: {int(actual):,}')\n",
        "print(f'predicted: {int(prediction):,}')\n",
        "diff = int(abs(prediction-actual))\n",
        "print(f'difference: {diff:,}')\n",
        "percent_diff = int((diff/actual)*100)\n",
        "print(f'percent difference: {percent_diff:,}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}